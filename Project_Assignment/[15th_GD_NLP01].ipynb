{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a8ff56c",
   "metadata": {},
   "source": [
    "# 프로젝트: 작사가 인공지능 만들기\n",
    "\n",
    "## 데이터:\n",
    "1Mb 남짓한 노래 가사 텍스트 파일\n",
    "\n",
    "## 목표:\n",
    "1. 특수문자 제거, 토크나이저 생성, 패딩 처리의 작업들이 빠짐없이 진행되었는가?\n",
    "2. 텍스트 제너레이션 결과로 생성된 문장이 해석 가능한 문장인가?\n",
    "3. 텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc04ae",
   "metadata": {},
   "source": [
    "## (1) 라이브러리 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41d4c96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import glob  #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd6e98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['', '', '[Spoken Intro:]']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*' \n",
    "txt_list = glob.glob(txt_file_path) \n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() \n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349c7f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not warm when she's away \n"
     ]
    }
   ],
   "source": [
    "# 문장 확인\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜀\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜀\n",
    "    if idx > 15: break   # 문장 15개 미만만 확인\n",
    "        \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df399919",
   "metadata": {},
   "source": [
    "## (2) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28496a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장 필터링\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "#     2. 특수문자 양쪽에 공백 넣기\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꾸기\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸기\n",
    "#     5. 다시 양쪽 공백을 지우기\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 필터링 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8800ee18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> spoken intro <end>',\n",
       " '<start> you ever want something <end>',\n",
       " '<start> that you know you shouldn t have <end>',\n",
       " '<start> the more you know you shouldn t have it , <end>',\n",
       " '<start> the more you want it <end>',\n",
       " '<start> and then one day you get it , <end>',\n",
       " '<start> it s so good too <end>',\n",
       " '<start> but it s just like my girl <end>',\n",
       " '<start> when she s around me <end>',\n",
       " '<start> i just feel so good , so good <end>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제된 문장 모으기\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 앞서 구현한 preprocess_sentence() 함수를 이용\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37a3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 2701 2584 ...    0    0    0]\n",
      " [   2    7  156 ...    0    0    0]\n",
      " [   2   17    7 ...    0    0    0]\n",
      " ...\n",
      " [   2  311    1 ...    0    0    0]\n",
      " [   5   34   45 ... 1161  143    3]\n",
      " [   5   34   45 ... 1161  143    3]] <keras_preprocessing.text.Tokenizer object at 0x7ffb5416f0d0>\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences 사용\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    # 7000단어를 기억할 수 있는 tokenizer를 만들 것임\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없음\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀 것임\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus) # 리스트의 형태로 변환 \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  #숫자의 시퀀스 형태로 변환\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)  \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춤\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28ccaa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 2701 2584    3    0    0    0    0    0    0]\n",
      " [   2    7  156   62  199    3    0    0    0    0]\n",
      " [   2   17    7   34    7 1518   15   76    3    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4780ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 만들기\n",
    "# 현재 계산된 단어의 인덱스와 인덱스에 해당하는 단어를 dictionary 형대로 반환 \n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33d86d",
   "metadata": {},
   "source": [
    "## (3) 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e1003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 2701 2584    3    0    0    0    0    0    0    0    0    0    0]\n",
      "[2701 2584    3    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "498ad00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer.num_words: 주어진 데이터의 문장들에서 빈도수가 높은 n개의 단어만 선택\n",
    " # tokenize() 함수에서 선언한 대로tokenizer.num_words의 값은 <pad>를 제외한 7000개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 데이터셋 제작 \n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7baa19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, train 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터셋을 입력하고, 테스트 세트의 비율을 설정\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b697013",
   "metadata": {},
   "source": [
    "## (4) 모델 구성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "752a39fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "embedding_size = 256 # 워드 벡터의 차원수 추상적으로 표현되는 크기\n",
    "hidden_size = 1024 # 모델에 얼마나 많은 일꾼을 둘 것인가? \n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) \n",
    "# pad 때문에 +1추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dbb0d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 7001), dtype=float32, numpy=\n",
       "array([[[ 3.34121461e-04,  3.28293718e-05, -6.17013138e-05, ...,\n",
       "         -1.52983193e-04,  2.65520357e-04, -1.16119363e-05],\n",
       "        [ 3.01958440e-04,  2.49672827e-04, -4.17649353e-05, ...,\n",
       "         -2.82788766e-04,  3.45869747e-04, -1.62855198e-04],\n",
       "        [ 3.43743479e-04,  5.39079891e-04, -9.35623757e-05, ...,\n",
       "         -5.42521360e-04,  6.14905730e-04, -5.02781593e-04],\n",
       "        ...,\n",
       "        [ 1.72232732e-03,  3.87319044e-04,  3.58989550e-04, ...,\n",
       "         -1.38842047e-03,  9.69737535e-04,  9.59633733e-04],\n",
       "        [ 1.94184284e-03,  4.64367389e-04,  1.60526906e-04, ...,\n",
       "         -1.70378329e-03,  1.04259909e-03,  4.95854532e-04],\n",
       "        [ 2.13912828e-03,  5.83029003e-04, -4.41790326e-05, ...,\n",
       "         -2.05494836e-03,  1.10507768e-03,  3.49157472e-06]],\n",
       "\n",
       "       [[ 3.34121461e-04,  3.28293718e-05, -6.17013138e-05, ...,\n",
       "         -1.52983193e-04,  2.65520357e-04, -1.16119363e-05],\n",
       "        [ 5.05282835e-04,  3.70739115e-04,  4.59623425e-06, ...,\n",
       "         -2.41922564e-04,  1.97162837e-04, -2.76942465e-05],\n",
       "        [ 4.76107292e-04,  5.95294812e-04, -3.44158849e-04, ...,\n",
       "         -2.91084289e-04,  4.17907024e-04, -1.64400029e-04],\n",
       "        ...,\n",
       "        [ 4.76890767e-04,  7.85107492e-04, -1.10452506e-03, ...,\n",
       "         -2.01643421e-03,  1.55792886e-03,  4.08784312e-04],\n",
       "        [ 7.29358639e-04,  7.24765705e-04, -1.01479248e-03, ...,\n",
       "         -2.07913248e-03,  1.56391808e-03,  3.47683177e-04],\n",
       "        [ 1.05077669e-03,  6.84155326e-04, -9.69566521e-04, ...,\n",
       "         -2.18088552e-03,  1.55005394e-03,  1.33012843e-04]],\n",
       "\n",
       "       [[ 3.34121461e-04,  3.28293718e-05, -6.17013138e-05, ...,\n",
       "         -1.52983193e-04,  2.65520357e-04, -1.16119363e-05],\n",
       "        [ 5.07209741e-04,  2.22234696e-04, -1.78710237e-04, ...,\n",
       "         -4.38115851e-04,  6.49587892e-04, -2.87042349e-04],\n",
       "        [ 3.91500711e-04,  3.89852357e-04, -6.35150005e-04, ...,\n",
       "         -3.89190245e-04,  8.48003780e-04, -3.18217470e-04],\n",
       "        ...,\n",
       "        [ 1.36858074e-03,  1.07626501e-03, -1.99165917e-03, ...,\n",
       "         -2.19544349e-03,  5.03345451e-04, -3.89399393e-05],\n",
       "        [ 1.72201311e-03,  1.10126240e-03, -1.94834475e-03, ...,\n",
       "         -2.46324763e-03,  5.86033741e-04, -4.01954225e-04],\n",
       "        [ 2.01855297e-03,  1.17077189e-03, -1.90971151e-03, ...,\n",
       "         -2.75828200e-03,  6.76038733e-04, -7.92565173e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.34121461e-04,  3.28293718e-05, -6.17013138e-05, ...,\n",
       "         -1.52983193e-04,  2.65520357e-04, -1.16119363e-05],\n",
       "        [ 6.29414513e-04,  9.57458469e-05, -6.98004878e-05, ...,\n",
       "         -9.86889572e-05,  5.43485512e-04, -3.14863137e-04],\n",
       "        [ 9.13509575e-04, -2.04511205e-04,  2.66098010e-04, ...,\n",
       "         -5.36242675e-04,  5.63890964e-04, -4.90262988e-04],\n",
       "        ...,\n",
       "        [ 2.06059450e-03,  1.15134811e-03, -4.06569394e-04, ...,\n",
       "         -2.26321467e-03,  6.12106640e-04, -1.36407453e-03],\n",
       "        [ 2.31470098e-03,  1.23183220e-03, -5.57725725e-04, ...,\n",
       "         -2.60920520e-03,  7.15189497e-04, -1.65529188e-03],\n",
       "        [ 2.52606231e-03,  1.32150587e-03, -7.03483587e-04, ...,\n",
       "         -2.95517314e-03,  8.22235597e-04, -1.94692612e-03]],\n",
       "\n",
       "       [[ 3.34121461e-04,  3.28293718e-05, -6.17013138e-05, ...,\n",
       "         -1.52983193e-04,  2.65520357e-04, -1.16119363e-05],\n",
       "        [ 6.52263698e-04,  1.91804793e-04,  2.05978649e-04, ...,\n",
       "         -2.47082702e-04,  1.98502355e-04, -1.18720855e-04],\n",
       "        [ 7.97673012e-04,  2.20446062e-04,  4.54599183e-04, ...,\n",
       "         -4.57815331e-04,  2.19539783e-04, -2.55308434e-04],\n",
       "        ...,\n",
       "        [ 1.34524005e-03,  7.73517182e-04, -1.07946224e-03, ...,\n",
       "         -2.40517687e-03,  1.01660483e-03, -1.12071738e-03],\n",
       "        [ 1.59911125e-03,  9.06444504e-04, -1.22388080e-03, ...,\n",
       "         -2.74558552e-03,  1.09521218e-03, -1.46447972e-03],\n",
       "        [ 1.83424947e-03,  1.04976445e-03, -1.34607777e-03, ...,\n",
       "         -3.08644306e-03,  1.16595242e-03, -1.79986667e-03]],\n",
       "\n",
       "       [[ 3.34121461e-04,  3.28293718e-05, -6.17013138e-05, ...,\n",
       "         -1.52983193e-04,  2.65520357e-04, -1.16119363e-05],\n",
       "        [ 2.97060760e-04,  3.27024987e-04,  5.76244292e-06, ...,\n",
       "         -2.14927044e-04,  2.95257982e-04, -4.19867501e-05],\n",
       "        [ 3.93172086e-04,  4.38857649e-04,  1.21472796e-04, ...,\n",
       "         -2.32109349e-04,  2.93427729e-04,  1.98798123e-04],\n",
       "        ...,\n",
       "        [ 1.75178063e-03,  9.67577158e-04, -7.25552090e-04, ...,\n",
       "         -1.58915448e-03,  1.53742812e-03,  5.82810608e-04],\n",
       "        [ 2.01332779e-03,  1.00189901e-03, -8.58740590e-04, ...,\n",
       "         -1.89053454e-03,  1.48548430e-03,  2.91878008e-04],\n",
       "        [ 2.26285681e-03,  1.06732536e-03, -9.75260453e-04, ...,\n",
       "         -2.21843715e-03,  1.43435819e-03, -6.93890834e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어 보기\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "074e45af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 구조를 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a8b2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일과 학습\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, # 기본값은 False, True = softmax함수가 적용되지 않음 \n",
    "    reduction='none'  # 기본값은 SUM. 각자 나오는 값의 반환 원할 때 None을 사용\n",
    ")\n",
    "# 모델 컴파일\n",
    "model.compile(loss=loss, optimizer=optimizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72038a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 93s 136ms/step - loss: 2.3749\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 93s 135ms/step - loss: 2.2732\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 93s 135ms/step - loss: 2.1775\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 93s 135ms/step - loss: 2.0849\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 93s 135ms/step - loss: 1.9947\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 93s 135ms/step - loss: 1.9065\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 93s 135ms/step - loss: 1.8206\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 93s 135ms/step - loss: 1.7372\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 93s 135ms/step - loss: 1.6565\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 93s 135ms/step - loss: 1.5796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffa8e1cf610>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(dataset, batch_size = 128, epochs=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991ed94",
   "metadata": {},
   "source": [
    "## (5) 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a38a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장생성 함수 정의\n",
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15): # 디폴트값 <start>\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
    "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성성\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4 \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51402e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i want to love you p . y . t . <end> '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I want\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17ce05ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i like the way how you re kissin me <end> '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I like\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3159aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , liberian girl , <end> '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I love\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4ebd08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i hope you can take it <end> '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I hope\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63473d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i just can t stop loving you <end> '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I just\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7716037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i hate the headlines and the weather <end> '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I hate\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4495f689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> bad bad really , really bad <end> '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> bad\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e66247ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m sad , yeah , yeah , yeah <end> '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I m sad\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f45f33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you look so good <end> '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you look\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d265c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> we re gonna have to say goodbye <end> '"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> we\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbf33c",
   "metadata": {},
   "source": [
    "-> 전반적으로 가사에 쓰기에 자연스러운 문장들이 잘 만들어졌다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2117e",
   "metadata": {},
   "source": [
    "# 프로젝트 회고\n",
    "\n",
    "## 배운점\n",
    "1. strip() 함수는 문자열에서 양쪽 끝에 있는 공백이나 특정 문자를 제거하는 데 사용된다.  \n",
    "2. tf.keras.preprocessing.text.Tokenizer를 사용하면 쉽게 토큰화 할 수 있다. \n",
    "\n",
    "## 아쉬운점\n",
    "\n",
    "## 종합평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086fbdda",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. model.fit 가이드  \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039abd95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
